{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "The preprocessing phase transformed raw Transport for London (TfL) passenger data and London weather data into a clean, feature-rich dataset suitable for modeling.\n",
    "\n",
    "The process began with loading TfL journey data from separate CSV files spanning 2019-2023, combining tube and bus journey counts into a total passenger count metric. Simultaneously, London weather data from 1979-2023 was imported to provide environmental context for passenger behavior analysis.\n",
    "\n",
    "Data quality was addressed through multiple cleaning operations. Dates were standardized to datetime format with invalid entries removed. Missing passenger counts were imputed using monthly averages to maintain temporal patterns. Extreme outliers were identified and filtered using z-score methodology (z > 3) to prevent them from skewing the model. For weather data, misleading rows with quality flags 1 and 9 were removed, and missing values were filled using monthly averages to preserve seasonal patterns. Snow depth and global radiation variables were excluded due to incomplete data coverage.\n",
    "\n",
    "To enable meaningful analysis, the TfL data was aggregated to daily totals and merged with weather data based on date alignment. The combined dataset was then split into training (2019-2022) and testing (2023) sets to facilitate model evaluation on unseen data.\n",
    "\n",
    "The dataset was enriched through feature engineering, adding temporal context through day-of-week and month indicators, and creating a binary feature for rainy days (precipitation > 0). Missing values resulting from lag/rolling features were handled using backward fill to maintain data continuity.\n",
    "\n",
    "To prevent multicollinearity issues, highly correlated weather features (correlation > 0.8) were identified and removed, specifically min_temp, mean_temp, and sunshine, which showed strong correlation with other variables. Finally, numerical features were normalized using StandardScaler to ensure consistent scale across variables, particularly for passenger_count, max_temp, precipitation, pressure, humidity, and cloud_cover.\n",
    "\n",
    "The final processed datasets contain the following features:\n",
    "\n",
    "| Feature | Description | Type |\n",
    "|---------|-------------|------|\n",
    "| date | Calendar date | datetime |\n",
    "| passenger_count | Total daily TfL passengers (normalized) | float |\n",
    "| max_temp | Maximum daily temperature (normalized) | float |\n",
    "| precipitation | Daily precipitation amount (normalized) | float |\n",
    "| pressure | Atmospheric pressure (normalized) | float |\n",
    "| humidity | Relative humidity (normalized) | float |\n",
    "| cloud_cover | Cloud coverage (normalized) | float |\n",
    "| day_of_week | Day of week (0-6) | int |\n",
    "| month | Month of year (1-12) | int |\n",
    "| is_raining | Binary indicator for precipitation > 0 | int |\n",
    "\n",
    "The preprocessing resulted in a training datas\n"
   ],
   "id": "b85e14682c133b23"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:56.779776Z",
     "start_time": "2025-06-14T19:13:56.751892Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfa46494421c20fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:56.805681Z",
     "start_time": "2025-06-14T19:13:56.793790Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "ABS_PATH = 'Bayesian-analysis-of-public-transport-passengers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "112ab2313ce675cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:56.986490Z",
     "start_time": "2025-06-14T19:13:56.906565Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "# TfL data: separate files for each year (2019â€“2023)\n",
    "tfl_files = {\n",
    "    2019: \"raw_data/Journeys_2019.csv\",\n",
    "    2020: \"raw_data/Journeys_2020.csv\",\n",
    "    2021: \"raw_data/Journeys_2021.csv\",\n",
    "    2022: \"raw_data/Journeys_2022.csv\",\n",
    "    2023: \"raw_data/Journeys_2023.csv\"\n",
    "}\n",
    "\n",
    "# Initialize empty list to store TfL data\n",
    "tfl_dfs = []\n",
    "\n",
    "# Load and concatenate TfL files\n",
    "for year, file in tfl_files.items():\n",
    "    file_path = os.path.join(ABS_PATH, file)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Sum TubeJourneyCount and BusJourneyCount into total passenger_count\n",
    "        df['passenger_count'] = df['TubeJourneyCount'] + df['BusJourneyCount']\n",
    "        # Keep only TravelDate and passenger_count\n",
    "        df = df[['TravelDate', 'passenger_count']]\n",
    "        tfl_dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: File {file} not found. Skipping...\")\n",
    "\n",
    "# Combine TfL data\n",
    "if tfl_dfs:\n",
    "    tfl_data = pd.concat(tfl_dfs, ignore_index=True)\n",
    "else:\n",
    "    raise FileNotFoundError(\"No TfL data files were found.\")\n",
    "\n",
    "# Load weather data\n",
    "weather_data = pd.read_csv(os.path.join(ABS_PATH,\"raw_data/london_weather_data_1979_to_2023.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "533d8568c918c56c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:57.063737Z",
     "start_time": "2025-06-14T19:13:57.030557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfL data: 1826 rows before dropping NaT\n",
      "TfL data: 1826 rows after dropping NaT\n",
      "TfL data after outlier removal: 1826 rows\n"
     ]
    }
   ],
   "source": [
    "# 2. Data Cleaning (TfL Data)\n",
    "# Convert date to datetime\n",
    "tfl_data['TravelDate'] = pd.to_datetime(tfl_data['TravelDate'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "# Check for invalid dates\n",
    "print(f\"TfL data: {len(tfl_data)} rows before dropping NaT\")\n",
    "tfl_data = tfl_data.dropna(subset=['TravelDate'])\n",
    "print(f\"TfL data: {len(tfl_data)} rows after dropping NaT\")\n",
    "\n",
    "# Handle missing passenger counts\n",
    "tfl_data['passenger_count'] = tfl_data['passenger_count'].fillna(tfl_data.groupby(tfl_data['TravelDate'].dt.month)['passenger_count'].transform('mean'))\n",
    "\n",
    "# Remove outliers (z-score > 3)\n",
    "z_scores = np.abs((tfl_data['passenger_count'] - tfl_data['passenger_count'].mean()) / tfl_data['passenger_count'].std())\n",
    "tfl_data = tfl_data[z_scores < 3]\n",
    "print(f\"TfL data after outlier removal: {len(tfl_data)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86fcfe8fde97e129",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:57.155342Z",
     "start_time": "2025-06-14T19:13:57.100954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather data: 16436 rows before deleting missleading rows\n",
      "Weather data: 14112 rows after  deleting missleading rows\n",
      "Weather data: 14112 rows before dropping NaT\n",
      "Weather data: 14112 rows after dropping NaT\n",
      "Weather data after cleaning: 14112 rows\n"
     ]
    }
   ],
   "source": [
    "# 3. Data Cleaning (Weather Data)\n",
    "\n",
    "#Deleting wrong or missleading rows and Q_ columns\n",
    "q_columns = [col for col in weather_data.columns if col.startswith('Q_')]\n",
    "print(f\"Weather data: {len(weather_data)} rows before deleting missleading rows\")\n",
    "weather_data = weather_data[~weather_data[q_columns].isin([1, 9]).any(axis=1)]\n",
    "print(f\"Weather data: {len(weather_data)} rows after  deleting missleading rows\")\n",
    "\n",
    "weather_data = weather_data.drop(columns=[col for col in weather_data.columns if col.startswith('Q_')])\n",
    "\n",
    "# Rename columns to match naming convention\n",
    "weather_data = weather_data.rename(columns={\n",
    "    'DATE': 'date',\n",
    "    'TX': 'max_temp',\n",
    "    'TN': 'min_temp',\n",
    "    'TG': 'mean_temp',\n",
    "    'SS': 'sunshine',\n",
    "    'SD': 'snow_depth',\n",
    "    'RR': 'precipitation',\n",
    "    'QQ': 'global_radiation',\n",
    "    'PP': 'pressure',\n",
    "    'HU': 'humidity',\n",
    "    'CC': 'cloud_cover'\n",
    "})\n",
    "\n",
    "# Convert date to datetime\n",
    "weather_data['date'] = pd.to_datetime(weather_data['date'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "# Check for invalid dates\n",
    "print(f\"Weather data: {len(weather_data)} rows before dropping NaT\")\n",
    "weather_data = weather_data.dropna(subset=['date'])\n",
    "print(f\"Weather data: {len(weather_data)} rows after dropping NaT\")\n",
    "\n",
    "# Handle missing weather values\n",
    "weather_columns = ['max_temp', 'min_temp', 'mean_temp', 'sunshine', 'precipitation',\n",
    "                   'pressure', 'humidity', 'cloud_cover']\n",
    "for col in weather_columns:\n",
    "    weather_data[col] = weather_data[col].fillna(weather_data.groupby(weather_data['date'].dt.month)[col].transform('mean'))\n",
    "weather_data = weather_data.drop(columns=['snow_depth', 'global_radiation'])  # Remove columns not used in analysis\n",
    "# Convert units\n",
    "'''weather_data['max_temp'] = weather_data['max_temp'] / 10\n",
    "weather_data['min_temp'] = weather_data['min_temp'] / 10\n",
    "weather_data['mean_temp'] = weather_data['mean_temp'] / 10\n",
    "weather_data['sunshine'] = weather_data['sunshine'] / 10\n",
    "weather_data['precipitation'] = weather_data['precipitation'] / 10\n",
    "weather_data['pressure'] = weather_data['pressure'] / 10'''\n",
    "\n",
    "# Remove invalid entries\n",
    "weather_data = weather_data[weather_data['precipitation'] >= 0]\n",
    "print(f\"Weather data after cleaning: {len(weather_data)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f048d0b23055cc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:57.217859Z",
     "start_time": "2025-06-14T19:13:57.194722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data: 1636 rows\n",
      "Train data (2019â€“2022): 1298 rows\n",
      "Test data (2023): 338 rows\n"
     ]
    }
   ],
   "source": [
    "# 4. Temporal Alignment\n",
    "# Aggregate TfL data to daily total\n",
    "tfl_data = tfl_data.rename(columns={'TravelDate': 'date'})\n",
    "daily_tfl = tfl_data.groupby('date')['passenger_count'].sum().reset_index()\n",
    "\n",
    "# Merge with weather data\n",
    "merged_data = pd.merge(daily_tfl, weather_data, on='date', how='inner')\n",
    "print(f\"Merged data: {len(merged_data)} rows\")\n",
    "\n",
    "# Filter for 2019â€“2022 (training) and 2023 (testing)\n",
    "train_data = merged_data[merged_data['date'].dt.year.isin([2019, 2020, 2021, 2022])]\n",
    "test_data = merged_data[merged_data['date'].dt.year == 2023]\n",
    "print(f\"Train data (2019â€“2022): {len(train_data)} rows\")\n",
    "print(f\"Test data (2023): {len(test_data)} rows\")\n",
    "\n",
    "# Check if train_data is empty\n",
    "if len(train_data) == 0:\n",
    "    raise ValueError(\"Train data is empty. Check date ranges in TfL and weather data for overlap in 2019â€“2022.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddb4b28b064e8bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:57.287408Z",
     "start_time": "2025-06-14T19:13:57.255446Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102/3651289884.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['date'] = pd.to_datetime(train_data['date'])\n",
      "/tmp/ipykernel_102/3651289884.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['date'] = pd.to_datetime(test_data['date'])\n",
      "/tmp/ipykernel_102/3651289884.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['day_of_week'] = train_data['date'].dt.dayofweek\n",
      "/tmp/ipykernel_102/3651289884.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['month'] = train_data['date'].dt.month\n",
      "/tmp/ipykernel_102/3651289884.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['is_raining'] = (train_data['precipitation'] > 0).astype(int)\n",
      "/tmp/ipykernel_102/3651289884.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['day_of_week'] = test_data['date'].dt.dayofweek\n",
      "/tmp/ipykernel_102/3651289884.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['month'] = test_data['date'].dt.month\n",
      "/tmp/ipykernel_102/3651289884.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['is_raining'] = (test_data['precipitation'] > 0).astype(int)\n",
      "/tmp/ipykernel_102/3651289884.py:18: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_data = train_data.fillna(method='bfill')\n",
      "/tmp/ipykernel_102/3651289884.py:20: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_data = test_data.fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "# 5. Feature Engineering\n",
    "# Ensure date is datetime\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "\n",
    "# Add temporal features for train_data\n",
    "train_data['day_of_week'] = train_data['date'].dt.dayofweek\n",
    "train_data['month'] = train_data['date'].dt.month\n",
    "train_data['is_raining'] = (train_data['precipitation'] > 0).astype(int)\n",
    "\n",
    "# Add temporal features for test_data (skip if empty)\n",
    "if len(test_data) > 0:\n",
    "    test_data['day_of_week'] = test_data['date'].dt.dayofweek\n",
    "    test_data['month'] = test_data['date'].dt.month\n",
    "    test_data['is_raining'] = (test_data['precipitation'] > 0).astype(int)\n",
    "\n",
    "# Handle missing values from lag/rolling features\n",
    "train_data = train_data.fillna(method='bfill')\n",
    "if len(test_data) > 0:\n",
    "    test_data = test_data.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a47092d973bd0438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:57.338678Z",
     "start_time": "2025-06-14T19:13:57.329616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns due to high correlation: ['min_temp', 'mean_temp', 'min_temp', 'sunshine']\n"
     ]
    }
   ],
   "source": [
    "# 6. Handle Redundancies\n",
    "# Drop highly correlated weather features\n",
    "corr_matrix = train_data[weather_columns].corr()\n",
    "high_corr = [(col1, col2) for col1 in corr_matrix.columns for col2 in corr_matrix.index\n",
    "             if col1 < col2 and abs(corr_matrix.loc[col2, col1]) > 0.8]\n",
    "if high_corr:\n",
    "    drop_cols = [col2 for col1, col2 in high_corr]\n",
    "    print(f\"Dropped columns due to high correlation: {drop_cols}\")\n",
    "    train_data = train_data.drop(columns=drop_cols)\n",
    "    if len(test_data) > 0:\n",
    "        test_data = test_data.drop(columns=drop_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0818da90fd86e61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:57.395463Z",
     "start_time": "2025-06-14T19:13:57.378670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns for normalization: ['passenger_count', 'max_temp', 'precipitation', 'pressure', 'humidity', 'cloud_cover']\n"
     ]
    }
   ],
   "source": [
    "# 7. Normalize Numerical Features\n",
    "# Dynamically select numerical columns that exist\n",
    "base_numerical_cols = ['passenger_count', 'max_temp', 'min_temp', 'mean_temp', 'precipitation',\n",
    "                       'sunshine', 'pressure', 'humidity', 'cloud_cover']\n",
    "numerical_cols = [col for col in base_numerical_cols if col in train_data.columns]\n",
    "print(f\"Numerical columns for normalization: {numerical_cols}\")\n",
    "\n",
    "# Apply normalization only if train_data is not empty\n",
    "if len(train_data) > 0:\n",
    "    scaler = StandardScaler()\n",
    "    train_data[numerical_cols] = scaler.fit_transform(train_data[numerical_cols])\n",
    "    if len(test_data) > 0:\n",
    "        test_data[numerical_cols] = scaler.transform(test_data[numerical_cols])\n",
    "else:\n",
    "    raise ValueError(\"Cannot normalize: train_data is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ba960095e11f560",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:57.470824Z",
     "start_time": "2025-06-14T19:13:57.432647Z"
    }
   },
   "outputs": [],
   "source": [
    "# 8. Save Processed Data\n",
    "train_data.to_csv(os.path.join(ABS_PATH,'processed_train_data.csv'), index=False)\n",
    "if len(test_data) > 0:\n",
    "    test_data.to_csv(os.path.join(ABS_PATH,'processed_test_data.csv'), index=False)\n",
    "else:\n",
    "    print(\"Warning: test_data is empty, skipping save of test_data CSV.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
