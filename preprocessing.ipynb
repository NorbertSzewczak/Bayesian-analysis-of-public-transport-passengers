{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:56.779776Z",
     "start_time": "2025-06-14T19:13:56.751892Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:56.805681Z",
     "start_time": "2025-06-14T19:13:56.793790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ],
   "id": "dfa46494421c20fd",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:56.986490Z",
     "start_time": "2025-06-14T19:13:56.906565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load Data\n",
    "# TfL data: separate files for each year (2019–2023)\n",
    "tfl_files = {\n",
    "    2019: \"raw_data/Journeys_2019.csv\",\n",
    "    2020: \"raw_data/Journeys_2020.csv\",\n",
    "    2021: \"raw_data/Journeys_2021.csv\",\n",
    "    2022: \"raw_data/Journeys_2022.csv\",\n",
    "    2023: \"raw_data/Journeys_2023.csv\"\n",
    "}\n",
    "\n",
    "# Initialize empty list to store TfL data\n",
    "tfl_dfs = []\n",
    "\n",
    "# Load and concatenate TfL files\n",
    "for year, file in tfl_files.items():\n",
    "    if os.path.exists(file):\n",
    "        df = pd.read_csv(file)\n",
    "        # Sum TubeJourneyCount and BusJourneyCount into total passenger_count\n",
    "        df['passenger_count'] = df['TubeJourneyCount'] + df['BusJourneyCount']\n",
    "        # Keep only TravelDate and passenger_count\n",
    "        df = df[['TravelDate', 'passenger_count']]\n",
    "        tfl_dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: File {file} not found. Skipping...\")\n",
    "\n",
    "# Combine TfL data\n",
    "if tfl_dfs:\n",
    "    tfl_data = pd.concat(tfl_dfs, ignore_index=True)\n",
    "else:\n",
    "    raise FileNotFoundError(\"No TfL data files were found.\")\n",
    "\n",
    "# Load weather data\n",
    "weather_data = pd.read_csv(\"raw_data/london_weather_data_1979_to_2023.csv\")"
   ],
   "id": "112ab2313ce675cb",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:57.063737Z",
     "start_time": "2025-06-14T19:13:57.030557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Data Cleaning (TfL Data)\n",
    "# Convert date to datetime\n",
    "tfl_data['TravelDate'] = pd.to_datetime(tfl_data['TravelDate'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "# Check for invalid dates\n",
    "print(f\"TfL data: {len(tfl_data)} rows before dropping NaT\")\n",
    "tfl_data = tfl_data.dropna(subset=['TravelDate'])\n",
    "print(f\"TfL data: {len(tfl_data)} rows after dropping NaT\")\n",
    "\n",
    "# Handle missing passenger counts\n",
    "tfl_data['passenger_count'] = tfl_data['passenger_count'].fillna(tfl_data.groupby(tfl_data['TravelDate'].dt.month)['passenger_count'].transform('mean'))\n",
    "\n",
    "# Remove outliers (z-score > 3)\n",
    "z_scores = np.abs((tfl_data['passenger_count'] - tfl_data['passenger_count'].mean()) / tfl_data['passenger_count'].std())\n",
    "tfl_data = tfl_data[z_scores < 3]\n",
    "print(f\"TfL data after outlier removal: {len(tfl_data)} rows\")\n"
   ],
   "id": "533d8568c918c56c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfL data: 1826 rows before dropping NaT\n",
      "TfL data: 1826 rows after dropping NaT\n",
      "TfL data after outlier removal: 1826 rows\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:57.155342Z",
     "start_time": "2025-06-14T19:13:57.100954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Data Cleaning (Weather Data)\n",
    "# Rename columns to match naming convention\n",
    "weather_data = weather_data.rename(columns={\n",
    "    'DATE': 'date',\n",
    "    'TX': 'max_temp',\n",
    "    'TN': 'min_temp',\n",
    "    'TG': 'mean_temp',\n",
    "    'SS': 'sunshine',\n",
    "    'SD': 'snow_depth',\n",
    "    'RR': 'precipitation',\n",
    "    'QQ': 'global_radiation',\n",
    "    'PP': 'pressure',\n",
    "    'HU': 'humidity',\n",
    "    'CC': 'cloud_cover'\n",
    "})\n",
    "\n",
    "# Convert date to datetime\n",
    "weather_data['date'] = pd.to_datetime(weather_data['date'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "# Check for invalid dates\n",
    "print(f\"Weather data: {len(weather_data)} rows before dropping NaT\")\n",
    "weather_data = weather_data.dropna(subset=['date'])\n",
    "print(f\"Weather data: {len(weather_data)} rows after dropping NaT\")\n",
    "\n",
    "# Handle missing weather values\n",
    "weather_columns = ['max_temp', 'min_temp', 'mean_temp', 'sunshine', 'snow_depth', 'precipitation',\n",
    "                   'global_radiation', 'pressure', 'humidity', 'cloud_cover']\n",
    "for col in weather_columns:\n",
    "    weather_data[col] = weather_data[col].fillna(weather_data.groupby(weather_data['date'].dt.month)[col].transform('mean'))\n",
    "\n",
    "# Convert units\n",
    "weather_data['max_temp'] = weather_data['max_temp'] / 10\n",
    "weather_data['min_temp'] = weather_data['min_temp'] / 10\n",
    "weather_data['mean_temp'] = weather_data['mean_temp'] / 10\n",
    "weather_data['sunshine'] = weather_data['sunshine'] / 10\n",
    "weather_data['precipitation'] = weather_data['precipitation'] / 10\n",
    "weather_data['pressure'] = weather_data['pressure'] / 10\n",
    "\n",
    "# Remove invalid entries\n",
    "weather_data = weather_data[weather_data['precipitation'] >= 0]\n",
    "print(f\"Weather data after cleaning: {len(weather_data)} rows\")"
   ],
   "id": "86fcfe8fde97e129",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather data: 16436 rows before dropping NaT\n",
      "Weather data: 16436 rows after dropping NaT\n",
      "Weather data after cleaning: 16436 rows\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:57.217859Z",
     "start_time": "2025-06-14T19:13:57.194722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Temporal Alignment\n",
    "# Aggregate TfL data to daily total\n",
    "tfl_data = tfl_data.rename(columns={'TravelDate': 'date'})\n",
    "daily_tfl = tfl_data.groupby('date')['passenger_count'].sum().reset_index()\n",
    "\n",
    "# Merge with weather data\n",
    "merged_data = pd.merge(daily_tfl, weather_data, on='date', how='inner')\n",
    "print(f\"Merged data: {len(merged_data)} rows\")\n",
    "\n",
    "# Filter for 2019–2022 (training) and 2023 (testing)\n",
    "train_data = merged_data[merged_data['date'].dt.year.isin([2019, 2020, 2021, 2022])]\n",
    "test_data = merged_data[merged_data['date'].dt.year == 2023]\n",
    "print(f\"Train data (2019–2022): {len(train_data)} rows\")\n",
    "print(f\"Test data (2023): {len(test_data)} rows\")\n",
    "\n",
    "# Check if train_data is empty\n",
    "if len(train_data) == 0:\n",
    "    raise ValueError(\"Train data is empty. Check date ranges in TfL and weather data for overlap in 2019–2022.\")"
   ],
   "id": "f048d0b23055cc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data: 1826 rows\n",
      "Train data (2019–2022): 1461 rows\n",
      "Test data (2023): 365 rows\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:57.287408Z",
     "start_time": "2025-06-14T19:13:57.255446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. Feature Engineering\n",
    "# Ensure date is datetime\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "\n",
    "# Add temporal features for train_data\n",
    "train_data['day_of_week'] = train_data['date'].dt.dayofweek\n",
    "train_data['month'] = train_data['date'].dt.month\n",
    "train_data['season'] = train_data['month'].map({\n",
    "    1: 'Winter', 2: 'Winter', 3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "    6: 'Summer', 7: 'Summer', 8: 'Summer', 9: 'Autumn', 10: 'Autumn',\n",
    "    11: 'Autumn', 12: 'Winter'\n",
    "})\n",
    "train_data['is_raining'] = (train_data['precipitation'] > 0).astype(int)\n",
    "train_data['passenger_count_lag1'] = train_data['passenger_count'].shift(1)\n",
    "train_data['temp_7d_mean'] = train_data['mean_temp'].rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "# Add temporal features for test_data (skip if empty)\n",
    "if len(test_data) > 0:\n",
    "    test_data['day_of_week'] = test_data['date'].dt.dayofweek\n",
    "    test_data['month'] = test_data['date'].dt.month\n",
    "    test_data['season'] = test_data['month'].map({\n",
    "        1: 'Winter', 2: 'Winter', 3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "        6: 'Summer', 7: 'Summer', 8: 'Summer', 9: 'Autumn', 10: 'Autumn',\n",
    "        11: 'Autumn', 12: 'Winter'\n",
    "    })\n",
    "    test_data['is_raining'] = (test_data['precipitation'] > 0).astype(int)\n",
    "    test_data['passenger_count_lag1'] = test_data['passenger_count'].shift(1)\n",
    "    test_data['temp_7d_mean'] = test_data['mean_temp'].rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "# Handle missing values from lag/rolling features\n",
    "train_data = train_data.fillna(method='bfill')\n",
    "if len(test_data) > 0:\n",
    "    test_data = test_data.fillna(method='bfill')"
   ],
   "id": "ddb4b28b064e8bd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10068\\3666198065.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['date'] = pd.to_datetime(train_data['date'])\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10068\\3666198065.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['date'] = pd.to_datetime(test_data['date'])\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10068\\3666198065.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['day_of_week'] = train_data['date'].dt.dayofweek\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10068\\3666198065.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['month'] = train_data['date'].dt.month\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10068\\3666198065.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['season'] = train_data['month'].map({\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10068\\3666198065.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['is_raining'] = (train_data['precipitation'] > 0).astype(int)\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10068\\3666198065.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['passenger_count_lag1'] = train_data['passenger_count'].shift(1)\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10068\\3666198065.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['temp_7d_mean'] = train_data['mean_temp'].rolling(window=7, min_periods=1).mean()\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10068\\3666198065.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['day_of_week'] = test_data['date'].dt.dayofweek\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10068\\3666198065.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['month'] = test_data['date'].dt.month\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10068\\3666198065.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['season'] = test_data['month'].map({\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10068\\3666198065.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['is_raining'] = (test_data['precipitation'] > 0).astype(int)\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10068\\3666198065.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['passenger_count_lag1'] = test_data['passenger_count'].shift(1)\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10068\\3666198065.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['temp_7d_mean'] = test_data['mean_temp'].rolling(window=7, min_periods=1).mean()\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10068\\3666198065.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_data = train_data.fillna(method='bfill')\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10068\\3666198065.py:34: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_data = test_data.fillna(method='bfill')\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:57.338678Z",
     "start_time": "2025-06-14T19:13:57.329616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6. Handle Redundancies\n",
    "# Drop highly correlated weather features\n",
    "corr_matrix = train_data[weather_columns].corr()\n",
    "high_corr = [(col1, col2) for col1 in corr_matrix.columns for col2 in corr_matrix.index\n",
    "             if col1 < col2 and abs(corr_matrix.loc[col2, col1]) > 0.8]\n",
    "if high_corr:\n",
    "    drop_cols = [col2 for col1, col2 in high_corr]\n",
    "    print(f\"Dropped columns due to high correlation: {drop_cols}\")\n",
    "    train_data = train_data.drop(columns=drop_cols)\n",
    "    if len(test_data) > 0:\n",
    "        test_data = test_data.drop(columns=drop_cols)\n"
   ],
   "id": "a47092d973bd0438",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns due to high correlation: ['mean_temp', 'min_temp', 'sunshine']\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:57.395463Z",
     "start_time": "2025-06-14T19:13:57.378670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 7. Normalize Numerical Features\n",
    "# Dynamically select numerical columns that exist\n",
    "base_numerical_cols = ['passenger_count', 'max_temp', 'min_temp', 'mean_temp', 'precipitation',\n",
    "                       'sunshine', 'snow_depth', 'global_radiation', 'pressure', 'humidity',\n",
    "                       'cloud_cover', 'passenger_count_lag1', 'temp_7d_mean']\n",
    "numerical_cols = [col for col in base_numerical_cols if col in train_data.columns]\n",
    "print(f\"Numerical columns for normalization: {numerical_cols}\")\n",
    "\n",
    "# Apply normalization only if train_data is not empty\n",
    "if len(train_data) > 0:\n",
    "    scaler = MinMaxScaler()\n",
    "    train_data[numerical_cols] = scaler.fit_transform(train_data[numerical_cols])\n",
    "    if len(test_data) > 0:\n",
    "        test_data[numerical_cols] = scaler.transform(test_data[numerical_cols])\n",
    "else:\n",
    "    raise ValueError(\"Cannot normalize: train_data is empty.\")"
   ],
   "id": "a0818da90fd86e61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns for normalization: ['passenger_count', 'max_temp', 'precipitation', 'snow_depth', 'global_radiation', 'pressure', 'humidity', 'cloud_cover', 'passenger_count_lag1', 'temp_7d_mean']\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:57.470824Z",
     "start_time": "2025-06-14T19:13:57.432647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 8. Save Processed Data\n",
    "train_data.to_csv('processed_train_data.csv', index=False)\n",
    "if len(test_data) > 0:\n",
    "    test_data.to_csv('processed_test_data.csv', index=False)\n",
    "else:\n",
    "    print(\"Warning: test_data is empty, skipping save of test_data CSV.\")"
   ],
   "id": "4ba960095e11f560",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T19:13:57.515695Z",
     "start_time": "2025-06-14T19:13:57.509655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Output confirmation\n",
    "print(\"Processed data saved as 'processed_train_data.csv'\")\n",
    "if len(test_data) > 0:\n",
    "    print(\"Processed data saved as 'processed_test_data.csv'\")"
   ],
   "id": "deea423c24d1390f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved as 'processed_train_data.csv'\n",
      "Processed data saved as 'processed_test_data.csv'\n"
     ]
    }
   ],
   "execution_count": 83
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
