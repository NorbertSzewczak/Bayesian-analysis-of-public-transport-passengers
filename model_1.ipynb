{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from cmdstanpy import CmdStanModel\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import arviz as az"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load preprocessed data\n",
    "train_data = pd.read_csv('processed_train_data.csv')\n",
    "test_data = pd.read_csv('processed_test_data.csv')\n",
    "\n",
    "# Check if data is empty\n",
    "if len(train_data) == 0:\n",
    "    raise ValueError(\"Train data is empty. Check preprocessing step.\")\n",
    "if len(test_data) == 0:\n",
    "    print(\"Warning: Test data is empty. Predictions will be skipped.\")"
   ],
   "id": "86919d2371735843"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define features and target\n",
    "categorical_cols = ['season', 'is_raining', 'day_of_week', 'month']\n",
    "numerical_cols = [col for col in train_data.columns if col not in ['date', 'passenger_count', 'season', 'is_raining', 'day_of_week', 'month']]\n",
    "features = numerical_cols + categorical_cols\n",
    "target = 'passenger_count'"
   ],
   "id": "73385e9a086bc77d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare training and test sets\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "X_test = test_data[features] if len(test_data) > 0 else pd.DataFrame(columns=X_train.columns)\n",
    "y_test = test_data[target] if len(test_data) > 0 else pd.Series()"
   ],
   "id": "299e5a4a993cc2fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Preprocess categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols),\n",
    "        ('num', 'passthrough', numerical_cols)\n",
    "    ])\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test) if len(X_test) > 0 else np.array([])"
   ],
   "id": "a2bf749a072e55f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get feature names after encoding\n",
    "cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "feature_names = list(cat_feature_names) + numerical_cols"
   ],
   "id": "7cdfc85acd6b82b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prior predictive checks for parameters\n",
    "n_sim = 1000\n",
    "beta0_sim = np.random.normal(0, 5, n_sim)\n",
    "beta_sim = np.random.normal(0, 5, (n_sim, len(feature_names)))\n",
    "sigma_sim = np.abs(np.random.standard_cauchy(n_sim) * 2.5)\n",
    "print(\"Prior Parameter Checks:\")\n",
    "print(f\"beta0 range: [{beta0_sim.min():.2f}, {beta0_sim.max():.2f}]\")\n",
    "print(f\"beta range: [{beta_sim.min():.2f}, {beta_sim.max():.2f}]\")\n",
    "print(f\"sigma range: [{sigma_sim.min():.2f}, {sigma_sim.max():.2f}]\")"
   ],
   "id": "c0c6e71f825ac761"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prior predictive checks for measurements\n",
    "if len(X_train_transformed) > 0:\n",
    "    mu_sim = beta0_sim + X_train_transformed[:n_sim] @ beta_sim.T\n",
    "    y_sim = np.array([np.random.normal(mu_sim[i], sigma_sim[i]) for i in range(n_sim)])\n",
    "    print(f\"Prior Predictive Measurements range: [{y_sim.min():.2f}, {y_sim.max():.2f}]\")\n",
    "    plt.hist(y_sim.flatten(), bins=50, density=True)\n",
    "    plt.title('Prior Predictive Distribution of Passenger Counts')\n",
    "    plt.xlabel('Normalized Passenger Count')\n",
    "    plt.ylabel('Density')\n",
    "    plt.savefig('prior_predictive_measurements.png')\n",
    "    plt.close()"
   ],
   "id": "6691c1ee7f96ebbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare data for Stan\n",
    "stan_data = {\n",
    "    'N': len(X_train_transformed),\n",
    "    'K': X_train_transformed.shape[1],\n",
    "    'X': X_train_transformed,\n",
    "    'y': y_train.values,\n",
    "    'N_new': len(X_test_transformed) if len(X_test_transformed) > 0 else 0,\n",
    "    'X_new': X_test_transformed if len(X_test_transformed) > 0 else np.zeros((0, len(feature_names)))\n",
    "}"
   ],
   "id": "c3dde7d9dbc1b1b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define Stan model with log-likelihood for arviz\n",
    "stan_code = \"\"\"\n",
    "data {\n",
    "  int<lower=0> N;          // Number of training samples\n",
    "  int<lower=0> K;          // Number of features\n",
    "  matrix[N, K] X;          // Feature matrix\n",
    "  vector[N] y;             // Target variable\n",
    "  int<lower=0> N_new;      // Number of test samples\n",
    "  matrix[N_new, K] X_new;  // Test feature matrix\n",
    "}\n",
    "parameters {\n",
    "  real beta0;              // Intercept\n",
    "  vector[K] beta;          // Feature coefficients\n",
    "  real<lower=0> sigma;     // Noise SD\n",
    "}\n",
    "model {\n",
    "  vector[N] mu;\n",
    "  // Priors\n",
    "  beta0 ~ normal(0, 5);\n",
    "  beta ~ normal(0, 5);\n",
    "  sigma ~ cauchy(0, 2.5);\n",
    "\n",
    "  // Likelihood\n",
    "  for (n in 1:N) {\n",
    "    mu[n] = beta0 + dot_product(X[n], beta);\n",
    "  }\n",
    "  y ~ normal(mu, sigma);\n",
    "}\n",
    "generated quantities {\n",
    "  vector[N_new] y_pred;\n",
    "  vector[N] log_lik;\n",
    "  for (n in 1:N_new) {\n",
    "    y_pred[n] = normal_rng(dot_product(X_new[n], beta) + beta0, sigma);\n",
    "  }\n",
    "  for (n in 1:N) {\n",
    "    log_lik[n] = normal_lpdf(y[n] | dot_product(X[n], beta) + beta0, sigma);\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Save Stan model\n",
    "with open('linear_regression.stan', 'w') as f:\n",
    "    f.write(stan_code)"
   ],
   "id": "26ccf3ccf9093e09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compile and fit model\n",
    "model = CmdStanModel(stan_file='linear_regression.stan')\n",
    "fit = model.sample(data=stan_data, chains=4, iter_sampling=1000, iter_warmup=500, seed=42)"
   ],
   "id": "7bdb6aad1c1939d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check sampling diagnostics\n",
    "summary = fit.summary()\n",
    "\n",
    "# Print available columns in the summary DataFrame\n",
    "print(\"Summary DataFrame columns:\", summary.columns.tolist())\n",
    "\n",
    "rhat = summary['R_hat'].max()\n",
    "\n",
    "# Check if 'N_Eff' column exists before accessing it\n",
    "if 'N_Eff' in summary.columns:\n",
    "    n_eff = summary['N_Eff'].min()\n",
    "    print(f\"Sampling Diagnostics: Max R-hat = {rhat:.4f}, Min N_Eff = {n_eff:.0f}\")\n",
    "    if rhat > 1.1 or n_eff < 100:\n",
    "        print(\"Warning: Sampling issues detected. Consider increasing iter_sampling or adapt_delta.\")\n",
    "else:\n",
    "    print(f\"Sampling Diagnostics: Max R-hat = {rhat:.4f}. N_Eff column not found in summary.\")\n",
    "    if rhat > 1.1:\n",
    "         print(\"Warning: High R-hat detected. Sampling issues likely. Consider increasing iter_sampling or adapt_delta.\")"
   ],
   "id": "dcc5dea9b6108f98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Posterior predictive analysis\n",
    "if len(X_test_transformed) > 0:\n",
    "    y_pred = fit.stan_variable('y_pred').mean(axis=0)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Posterior Predictive Metrics:\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    # Check consistency\n",
    "    y_pred_samples = fit.stan_variable('y_pred')\n",
    "    credible_intervals = np.percentile(y_pred_samples, [2.5, 97.5], axis=0)\n",
    "    within_ci = np.mean((y_test >= credible_intervals[0]) & (y_test <= credible_intervals[1]))\n",
    "    print(f\"Proportion of test data within 95% credible intervals: {within_ci:.2f}\")\n",
    "    if within_ci < 0.9:\n",
    "        print(\"Warning: Less than 90% of test data within credible intervals. Model may miss non-linear effects or key features.\")"
   ],
   "id": "fcd19d5b8ed5a3ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Posterior predictive analysis\n",
    "if len(X_test_transformed) > 0:\n",
    "    y_pred = fit.stan_variable('y_pred').mean(axis=0)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Posterior Predictive Metrics (Model 1):\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    # Check consistency\n",
    "    y_pred_samples = fit.stan_variable('y_pred')\n",
    "    credible_intervals = np.percentile(y_pred_samples, [2.5, 97.5], axis=0)\n",
    "    within_ci = np.mean((y_test >= credible_intervals[0]) & (y_test <= credible_intervals[1]))\n",
    "    print(f\"Proportion of test data within 95% credible intervals: {within_ci:.2f}\")\n",
    "    if within_ci < 0.9:\n",
    "        print(\"Warning: Less than 90% of test data within credible intervals. Consider non-linear effects or additional features.\")\n",
    "\n",
    "    # Visualization: Actual vs. Predicted Passenger Counts\n",
    "    if 'date' in test_data.columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(test_data['date'], y_test, label='Actual', color='blue')\n",
    "        plt.plot(test_data['date'], y_pred, label='Predicted', color='red', alpha=0.7)\n",
    "        plt.title('Actual vs. Predicted Passenger Counts (Model 1)')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Normalized Passenger Count')\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('actual_vs_predicted_model1.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(y_test.index, y_test, label='Actual', color='blue')\n",
    "        plt.plot(y_test.index, y_pred, label='Predicted', color='red', alpha=0.7)\n",
    "        plt.title('Actual vs. Predicted Passenger Counts (Model 1)')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Normalized Passenger Count')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('actual_vs_predicted_model1.png')\n",
    "        plt.close()\n",
    "\n",
    "    # Visualization: Residuals Plot\n",
    "    residuals = y_test - y_pred\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(test_data['date'] if 'date' in test_data.columns else y_test.index, residuals, color='purple', alpha=0.5)\n",
    "    plt.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "    plt.title('Residuals of Predicted vs. Actual Passenger Counts (Model 1)')\n",
    "    plt.xlabel('Date' if 'date' in test_data.columns else 'Index')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('residuals_model1.png')\n",
    "    plt.close()"
   ],
   "id": "6233194731f6d9e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Parameter marginal distributions\n",
    "beta_samples = fit.stan_variable('beta')\n",
    "beta0_samples = fit.stan_variable('beta0')\n",
    "sigma_samples = fit.stan_variable('sigma')\n",
    "print(\"\\nParameter Summaries:\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    mean, std = beta_samples[:, i].mean(), beta_samples[:, i].std()\n",
    "    ci = np.percentile(beta_samples[:, i], [2.5, 97.5])\n",
    "    print(f\"{name}: Mean = {mean:.4f}, SD = {std:.4f}, 95% CI = [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
    "print(f\"beta0: Mean = {beta0_samples.mean():.4f}, SD = {beta0_samples.std():.4f}, 95% CI = [{np.percentile(beta0_samples, 2.5):.4f}, {np.percentile(beta0_samples, 97.5):.4f}]\")\n",
    "print(f\"sigma: Mean = {sigma_samples.mean():.4f}, SD = {sigma_samples.std():.4f}, 95% CI = [{np.percentile(sigma_samples, 2.5):.4f}, {np.percentile(sigma_samples, 97.5):.4f}]\")"
   ],
   "id": "f77daf0f66089da5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot parameter histograms\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, name in enumerate(feature_names[:4]):  # Plot first 4 for brevity\n",
    "    plt.subplot(10, 4, i+1)\n",
    "    plt.hist(beta_samples[:, i], bins=30, density=True)\n",
    "    plt.title(f'Posterior: {name}')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "plt.tight_layout()\n",
    "plt.savefig('parameter_histograms.png')\n",
    "plt.show()"
   ],
   "id": "4fa5fe74a5ab2315"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute information criteria with arviz using from_cmdstanpy\n",
    "idata = az.from_cmdstanpy(posterior=fit, log_likelihood='log_lik')\n",
    "waic = az.waic(idata)\n",
    "loo = az.loo(idata)\n",
    "print(f\"\\nInformation Criteria (Model 1):\")\n",
    "print(f\"WAIC: {waic.elpd_waic} (+/- {waic.se})\")\n",
    "print(f\"PSIS-LOO: {loo.elpd_loo} (+/- {loo.se})\")\n",
    "if any(loo.pareto_k > 0.7):\n",
    "    print(\"Warning: High Pareto k values detected. Results may be unreliable.\")"
   ],
   "id": "283f4037bbb70f95"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
